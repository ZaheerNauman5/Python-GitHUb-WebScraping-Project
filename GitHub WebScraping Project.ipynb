{"cells":[{"cell_type":"markdown","metadata":{"id":"ZsOXhf8RSib9"},"source":["#**GitHub WebScraping Project**"]},{"cell_type":"markdown","metadata":{"id":"WlXoLSHrSO6_"},"source":["##**Outline:**\n","####**This project has 2 phases.**\n","* **In phase 1 I have to scrape top 120 topic names ,their descriptions and their URLs from https://github.com/topics.**\n","* **In phase 2  for first 20 topic I have to scrape 100 top repos,repo username,repo title ,repo URLs and the no of stars of repo.**"]},{"cell_type":"markdown","metadata":{"id":"ouOTGnjnUSyQ"},"source":["##**Phase1:**\n","* **In this phase i will get a list of 100 topics.**\n","* **For each topic I will get topic title,topic descriptions, and topic page URL.**\n"]},{"cell_type":"markdown","metadata":{"id":"OgVYISqFUvU7"},"source":["##**Using the requests library to download web pages**\n","* I have to import requests library.\n","* Then i do get request to URL.\n","* In return i got all contents of URL webpage."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8-tfABvdVwKj"},"outputs":[],"source":["topic_url=\"https://github.com/topics?page=1\"\n","import requests\n","response=requests.get(topic_url)\n","page_contents=response.text"]},{"cell_type":"markdown","metadata":{"id":"TZhADMpWWthh"},"source":["###**Using BeautifulSoup to parse and extract information**\n","* Now i have to import BeautifulSoup library.\n","* After importing i parsed the content to the BeautifulSoup\n","* Then i extracted useful information from parsed content.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GMBUPlJuSTo4"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","doc=BeautifulSoup(page_contents,\"lxml\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"sIuLqxQlYwOI"},"outputs":[{"name":"stdout","output_type":"stream","text":["30\n"]}],"source":["#Getting list of Topics \n","topics=[]\n","topic_title_tags=doc.find_all(\"p\",class_=\"f3 lh-condensed mb-0 mt-1 Link--primary\")\n","print(len(topic_title_tags))\n","for tags in topic_title_tags:\n","  topics.append(tags.text)\n","# print(topics)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"TBQIev0XY_Vl"},"outputs":[],"source":["# Getting Descriptions of each topic\n","topic_descr=[]\n","topic_descr_tags=doc.find_all(\"p\",class_=\"f5 color-fg-muted mb-0 mt-1\")\n","for tags in topic_descr_tags:\n","  topic_descr.append(tags.text.strip())\n","# print(topic_descr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBdFJYKCZDOL"},"outputs":[],"source":["# Getting URL of the Topic page\n","topics_urls=[]\n","topic_url_tags=doc.find_all(\"a\",class_=\"no-underline flex-grow-0\")\n","for tags in topic_url_tags:\n","  url=(tags[\"href\"])\n","  # we need to add baseurl to these topic url so\n","  base_url=\"https://github.com\"\n","  topic_url=(base_url+url)\n","  topics_urls.append(topic_url)"]},{"cell_type":"markdown","metadata":{"id":"Rc0swZGSZX_p"},"source":["###**Creating a csv file with the Extracted infromation**\n","* Now i have stored all of extracted information into a csv file.\n","* for this purpose i have acreated a dictionary and give it all information.\n","* After that i have imported pandas library to convert this dictionary into a dataframe and then store it in a csv file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tj-HXFeaZcQB"},"outputs":[],"source":["topics_dict={\n","    \"Topic\": topics,\n","    \"Topic Description\":topic_descr ,\n","    \"Topic URL\": topics_urls ,\n","}\n","# now convert this dictionary into a DataFrame by using pandas library and export in csv\n","import pandas as pd \n","topic_df=pd.DataFrame(topics_dict,index=[i for i in range (1,31)])\n","# now extracting dataframe into csv file\n","topic_df.to_csv(\"Topics info.csv\")"]},{"cell_type":"markdown","metadata":{"id":"sjNvczHoatBz"},"source":["###**Combining all code and making a single Function**\n","* Now i am creating a single function that will take starting and ending page and extract all information of that pages into a csv files.\n","* After creating a single function i will be able to scrape unlimited no of pages into a single csv file.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooR95UNabOo4"},"outputs":[],"source":["def get_topic_info(startingpage,endingpage):\n","  all_df=[]\n","  for i in range (startingpage,endingpage+1):\n","    topic_url=\"https://github.com/topics?page={}\".format(i)\n","    response=requests.get(topic_url)\n","    page_contents=response.text\n","    from bs4 import BeautifulSoup\n","    doc=BeautifulSoup(page_contents,\"lxml\")\n","    #Getting list of Topics \n","    topics=[]\n","    topic_title_tags=doc.find_all(\"p\",class_=\"f3 lh-condensed mb-0 mt-1 Link--primary\")\n","    for tags in topic_title_tags:\n","      topics.append(tags.text)\n","    # Getting Descriptions of each topic\n","    topic_descr=[]\n","    topic_descr_tags=doc.find_all(\"p\",class_=\"f5 color-fg-muted mb-0 mt-1\")\n","    for tags in topic_descr_tags:\n","      topic_descr.append(tags.text.strip())\n","    # Getting URL of the Topic page\n","    topics_urls=[]\n","    topic_url_tags=doc.find_all(\"a\",class_=\"no-underline flex-grow-0\")\n","    for tags in topic_url_tags:\n","      url=(tags[\"href\"])\n","      base_url=\"https://github.com\"\n","      topic_url=(base_url+url)\n","      topics_urls.append(topic_url)\n","    # extracting all above informations into a csv file.\n","    # for this purpose first make a dictionary\n","    topics_dict={\n","        \"Topic\": topics,\n","        \"Topic Description\":topic_descr ,\n","        \"Topic URL\": topics_urls ,\n","    }\n","    # now convert this dictionary into a DataFrame by using pandas library and exporting in a csv file \n","    import pandas as pd \n","    topic_df=pd.DataFrame(topics_dict)\n","    all_df.append(topic_df)\n","    data_frame=pd.concat(all_df,ignore_index=True)\n","    # now extracting dataframe into csv file\n","    data_frame.to_csv(\"Topics info.csv\")\n","  print(\"Data Scraped Succesfuly\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0sw0C3vc4Wq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Scraped Succesfuly\n"]}],"source":["# invoking function and getting data of page 1 to 4.you can give any range.\n","get_topic_info(1,4)"]},{"cell_type":"markdown","metadata":{"id":"sYIP1TU5e6yn"},"source":["##**Phase2**\n","* **In this phase I will get 100 repos of first 20 topics.**\n","* **For 100 repos of first 20 topics ,I will extract username,repo name,repoURL and no of stars of each repo.**\n","* **After extracting info I will store all info into a  csv for each topic.**"]},{"cell_type":"markdown","metadata":{"id":"axItAQmgm5F7"},"source":["####first of all i get all info from first topic url and later i expanded it to all topic urls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXDrSyQbgpU5"},"outputs":[{"data":{"text/plain":["90000"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["topic_url=topics_urls[0]\n","#  using request library\n","response=requests.get(topic_url)\n","content=response.text\n","# parsing content to BeautifulSoup\n","doc=BeautifulSoup(content,\"lxml\")\n","# getting username,reponame,repoURL and repo stars\n","h3_tags=doc.find_all(\"h3\",class_=\"f3 color-fg-muted text-normal lh-condensed\")\n","a_tags=h3_tags[0].find_all(\"a\")\n","username=(a_tags[0].text.strip()) \n","repo_name=(a_tags[1].text.strip()) \n","repo_url=(base_url+a_tags[1][\"href\"])\n","star_tags=doc.find_all(\"span\",class_=\"Counter js-social-count\")\n","stars=(star_tags[0].text)\n","# creating a function that will no of stars in k and convert them into simple integer.\n","def parse_star_count(star_str):\n","  star_str=star_str.strip()\n","  if star_str[-1]==\"k\":\n","    no=int(float(star_str[:-1])*1000)\n","    return no  \n","  elif int(star_str) not in [i for i in range  (1,1000)]:\n","    return \"Not Available\"\n","  else: \n","    return star_str\n","parse_star_count(stars)   "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iw8hWgqjnuEx"},"source":[" #### Now i defined a function that will take h3 tags and star tags from list of h3 tags and star tag and extract useful info from these tags.\n","#### This function will return user name,repo name,stars,repo URL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8X52E-umKuD"},"outputs":[],"source":["def get_repo_info(h3_tags,star_tag):\n","  # return all the info about a reository\n","  a_tags=h3_tags.find_all(\"a\")\n","  username=(a_tags[0].text.strip()) \n","  repo_name=(a_tags[1].text.strip()) \n","  repo_url=(base_url+a_tags[1][\"href\"])\n","  stars=parse_star_count(star_tag.text)\n","  return username,repo_name,stars,repo_url"]},{"cell_type":"markdown","metadata":{"id":"JpPaIuuCosPb"},"source":["#### Now i defined a function that will use previous function(get_repo_info) and create a dictionary of all information\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3i_oGrq5mchF"},"outputs":[],"source":["topic_repos_dict={\n","    \"username\":[],\n","    \"repo_name\":[],\n","    \"stars\":[],\n","    \"repo_url\":[]\n","}\n","for i in range (len(h3_tags)):\n","  repo_info=get_repo_info(h3_tags[i],star_tags[i])\n","  topic_repos_dict[\"username\"].append(repo_info[0])\n","  topic_repos_dict[\"repo_name\"].append(repo_info[1])\n","  topic_repos_dict[\"stars\"].append(repo_info[2])\n","  topic_repos_dict[\"repo_url\"].append(repo_info[3])"]},{"cell_type":"markdown","metadata":{"id":"taal5Q4apH8j"},"source":["##**Combining all code into a single function**\n","* I created a function (get_topic_repos) that will take topic url and give all info of that repo in a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L6Vq3CC79U0"},"outputs":[],"source":["def get_topic_repos(topic_url):\n","  # download the page \n","  response=requests.get(topic_url)\n","  # check the response\n","  if response.status_code!=200:\n","    raise Exception(\"Failed to load page{}\".fromate(topic_url))\n","  page_content=response.text\n","  # parse using beautiful soup\n","  doc=BeautifulSoup(page_content,\"lxml\")\n","  # getting h3 tags containing username,reponame,repourl\n","  h3_tags=doc.find_all(\"h3\",class_=\"f3 color-fg-muted text-normal lh-condensed\")\n","  # finding startags containg no of stars\n","  star_tags=doc.find_all(\"span\",class_=\"Counter js-social-count\")\n","  # getting all of repo info\n","  topic_repos_dict={\n","    \"Username\":[],\n","    \"Repo_name\":[],\n","    \"Stars\":[],\n","    \"Repo_url\":[]}\n","  for i in range (len(h3_tags)):\n","    repo_info=get_repo_info(h3_tags[i],star_tags[i])\n","    topic_repos_dict[\"Username\"].append(repo_info[0])\n","    topic_repos_dict[\"Repo_name\"].append(repo_info[1])\n","    topic_repos_dict[\"Stars\"].append(repo_info[2])\n","    topic_repos_dict[\"Repo_url\"].append(repo_info[3])\n","  return pd.DataFrame(topic_repos_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaeplyx3pHg0"},"outputs":[],"source":["# checking our get_topic_repo function\n","# get_topic_repos(topics_urls[10]).to_csv(\"atom.csv\") #by giving any topic llink we can get its all repo information"]},{"cell_type":"markdown","metadata":{"id":"LIilz6eIAEvO"},"source":["# **Getting info of repos of every topic and storing them in separete csv files**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LLXNH6A9so7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sraping Topic no 1 with title 3D\n","Sraping Topic no 2 with title Ajax\n","Sraping Topic no 3 with title Algorithm\n","Sraping Topic no 4 with title Amp\n","Sraping Topic no 5 with title Android\n","Sraping Topic no 6 with title Angular\n","Sraping Topic no 7 with title Ansible\n","Sraping Topic no 8 with title API\n","Sraping Topic no 9 with title Arduino\n","Sraping Topic no 10 with title ASP.NET\n","Sraping Topic no 11 with title Atom\n","Sraping Topic no 12 with title Awesome Lists\n","Sraping Topic no 13 with title Amazon Web Services\n","Sraping Topic no 14 with title Azure\n","Sraping Topic no 15 with title Babel\n","Sraping Topic no 16 with title Bash\n","Sraping Topic no 17 with title Bitcoin\n","Sraping Topic no 18 with title Bootstrap\n","Sraping Topic no 19 with title Bot\n","Sraping Topic no 20 with title C\n","20 Topics scraped Sucesfully\n"]}],"source":["# getting info of repos of every topic and storing them in separete csv fillles\n","file_no=0\n","data_frame=pd.read_csv(\"Topics info.csv\")\n","for index,row in data_frame.iterrows():\n","  list_df=[]\n","  # print(row[\"Topic URL\"])\n","  file_no+=1\n","  print(\"Sraping Topic no {} with title {}\".format(file_no,row[\"Topic\"]))\n","  for i in range(1,6):\n","    df=get_topic_repos(row[\"Topic URL\"]+\"?page={}\".format(i))\n","    list_df.append(df)\n","  dm= pd.concat(list_df,ignore_index=True)\n","  dm.to_csv(\"{} Topicno {}.csv\".format(row[\"Topic\"],file_no))  \n","  if file_no==20:      # you can give no of your own choose where you want to stop\n","    break\n","print(\"{} Topics scraped Sucesfully\".format(file_no))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#**Conclusion:**\n","**This project was very interesting for me.I love to these web scarping project.I completed this project perfectly.**"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPdQLPgbEsHI8ND716Eu6Ue","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
